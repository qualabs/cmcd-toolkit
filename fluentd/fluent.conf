# Input plugin: Listen for events from Docker fluentd log driver
<source>
  @type forward
  @id input_docker
  port 24224
  bind 0.0.0.0
</source>

# Match logs from the collector service
<match docker.collector.**>
  @type copy
  <store>
    @type stdout # For debugging - logs to Fluentd's console
  </store>
  <store>
    @type influxdb2
    @id output_influxdb
    url "http://influxdb:8086"
    token "#{ENV['FLUENT_INFLUXDB_TOKEN']}" # Use environment variable for token
    organization "#{ENV['FLUENT_INFLUXDB_ORG']}"
    bucket "#{ENV['FLUENT_INFLUXDB_BUCKET']}"
    measurement "cmcd_data" # Define a measurement name
    # Available formatters: json, hash, msgpack. 'json' is suitable for structured logs.
    # If logs are JSON strings, they need to be parsed first.
    # Assuming the log from docker is a JSON string, we might need a parser.
    # For now, let's assume the fluentd log driver sends a structured payload.
    # If not, we'd add a parser block here.
    # <parse>
    #   @type json
    # </parse>
    # Fields to be stored as InfluxDB tags. Adjust as needed.
    # tag_keys ["object_id", "request_id"]
    # Fields to be stored as InfluxDB fields.
    # field_keys ["key1", "value1"]
    # If field_keys is empty or not specified, all fields are stored.
    # Use dot notation for nested JSON if applicable, e.g., "nested.key"
    # auto_tags true # if you want fluentd tags to become influxdb tags (like docker.collector.container_name)

    # Buffer settings
    flush_interval 10s
    # Other buffer settings can be added for robustness
  </store>
</match>

# Match any other logs and print to stdout (optional, for debugging)
<match **>
  @type stdout
  @id output_stdout_default
</match>
